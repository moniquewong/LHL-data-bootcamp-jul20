{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Time Series 1\n",
    "\n",
    "### September 14 | Week 9 Day 1\n",
    "### Instructor: Monique Wong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. What is time series?\n",
    "* Common techniques: \"statistical\" and supervised machine learning\n",
    "* Stastistical techniques:\n",
    "    - Decomposition\n",
    "    - Concept of stationarity\n",
    "    - Box-Jenkins / ARIMA methods\n",
    "    \n",
    "Very little code today - the goal is to build intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a time series?\n",
    "- When your data points are chronological (related to time), ordered\n",
    "- We can use the time dimension to better make predictions and forecasts\n",
    "- Same idea as before:\n",
    "    - We're trying to find a model that fits the data well\n",
    "    - This can be using statistical techniques or traditional machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common methods\n",
    "\n",
    "- Traditionally, the field if \"time series\" is about finding the best **statistical model** that fits the data\n",
    "    - This is what we'll focus on today and tomorrow\n",
    "    - The techniques we'll cover are **univariate**\n",
    "        - We are using past data of a particular variable to forecast the future data of the same variable\n",
    "        - E.g., Using past sales data for the last 7 years to forecast what sales will be for the next 3 months\n",
    "- You can also deal with time series with **supervised learning**\n",
    "    - We have to make sure that our train/test split \"follow the rules of time\"\n",
    "    - Our features are past data at different time steps\n",
    "    - Additional features can be added for **multivariate** use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are we trying to predict?\n",
    "- **Point estimate:** e.g., exactly how many units are being sold\n",
    "- **Confidence interval:** the 50%, 75%, 90% range of likely results\n",
    "\n",
    "**Discussion:** What is more helpful? What are some use cases for predicting a range?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series Methods: Decomposition\n",
    "- Aim to describe the seasonality and trend in the data\n",
    "\n",
    "| Component | Definition |\n",
    "|:- |:------- |\n",
    "| Trend | Long-term increase or decrease in the data, does not have to be linear |\n",
    "| Seasonal | Affected by seasonal factors like the time of the year or the day of the week, always a fixed and known frequency |\n",
    "| Cyclic | Time series that rises and falls, but not of fixed frequency, measured in years. |\n",
    "\n",
    "Let's look at some pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion: what components do we see here?\n",
    "\n",
    "<img src='imgs/trends-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion: what components do we see?\n",
    "\n",
    "| Chart | Seasonality | Cyclic | Trend |\n",
    "|:-|:---|:---|:---|\n",
    "| Monthly housing sales | Strong, within the year | Strong, 6-10 year cycles | No apparent trend |\n",
    "| US treasury bill contracts | None | N/A | Obvious downward trend, may be a part of a season / cycle|\n",
    "| Australian quarterly electricity production | Strong | No evidence | Strong increasing trend |\n",
    "| Daily change in the Google closing stock price | None | None | None |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The key idea: we can decompose each component\n",
    "\n",
    "### 1. Additive model\n",
    "\n",
    "$$ y_t = S_t + T_t + R_t$$\n",
    "\n",
    "\n",
    "### 2. Multiplicative model\n",
    "\n",
    "$$ y_t = S_t \\times T_t \\times R_t$$\n",
    "\n",
    "Where: \n",
    "- $S_t$ is the seasonal component\n",
    "- $T-t$ is the trend-cycle component\n",
    "- $R-t$ is the remainder component (whatever is left)\n",
    "\n",
    "### How to pick? \n",
    "- Whatever fits best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decomposition: easier to see in pictures...\n",
    "\n",
    "### Explaining the trend-cycle component\n",
    "<img src='imgs/elecequip-trend-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decomposition: easier to see in pictures...\n",
    "\n",
    "### The decomposition itself\n",
    "<img src='imgs/elecequip-stl-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decomposition: final notes\n",
    "\n",
    "\n",
    "### Briefly, the math\n",
    "- Mathematically, there are different ways to do the decomposition\n",
    "- They will result in slightly different results\n",
    "- Some examplies: Classical, X11, SEATS, STL\n",
    "- For more: https://otexts.com/fpp2/classical-decomposition.html\n",
    "\n",
    "\n",
    "### Forecasting\n",
    "- The idea here is simple\n",
    "- Future projections is the addition or multiplication (depending on additive or multiplicative model) of the seasonal component with \"everything else\"\n",
    "- \"Everything else\" will be modeled some other way - also called the seasonally-adjusted component\n",
    "    - Many methods here: Holt, ARIMA\n",
    "    - We'll cover ARIMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break\n",
    "\n",
    "We'll reconvene in ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series Methods: ARIMA models (Box-Jenkins)\n",
    "- Aim to describe the autocorrelations in the data\n",
    "- **Autocorrelations**: fancy word for how the next data points are correlated with previous data points\n",
    "- Cannot be applied to a time series with a seasonal component - **the seasonal component must be taken out first**\n",
    "\n",
    "\n",
    "### Before we explain the two parts of ARIMA (AR and MA), we need to understand stationarity...\n",
    "- AR (autoregressive) and MA (moving average) models only work with stationary data\n",
    "- The \"I\" stands for integrated: this is the differencing needed to turn a seasonally-adjusted data to stationary data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept of stationarity\n",
    "\n",
    "This is the first thing we need to understand well. In essence:\n",
    "> A stationary time series is one whose properties do not depend on the time at which the series is observed\n",
    "\n",
    "In less formal words:\n",
    "- There is no cyclical trend, seasonality trend, weekly trend etc.\n",
    "- It's \"white noise\"\n",
    "\n",
    "Let's take a look at some pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which of these are stationary?\n",
    "<img src='imgs/stationary-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which of these are stationary - discussion\n",
    "\n",
    "a. Google stock price for 200 consecutive days - long-term trend\n",
    "\n",
    "b. Daily change in the Google stock price for 200 consecutive days - **could be stationary**\n",
    "\n",
    "c. Annual number of strikes in the US - long-term trend\n",
    "\n",
    "d. Monthly sales of new one-family houses sold in the US - some seasonality\n",
    "\n",
    "e. Annual price of a dozen eggs in the US (constant dollars) - long-term trend\n",
    "\n",
    "f. Monthly total of pigs slaughtered in Victoria, Australia - long-term trend\n",
    "\n",
    "g. Annual total of lynx trapped in the McKenzie River district of north-west Canada - **could be stationary**\n",
    "\n",
    "h. Monthly Australian beer production - some seasonality\n",
    "\n",
    "i. Monthly Australian electricity production - some seasonality, long-term trend\n",
    "\n",
    "Source: https://otexts.com/fpp2/stationarity.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting to stationary data\n",
    "\n",
    "### At a high level:\n",
    "- The data minus seasonal component = seasonally adjusted data\n",
    "- Seasonally adjusted data with differencing = stationary data (hopefully)\n",
    "- Now, we can use ARMA (notice, no I) models to model the stationary data\n",
    "\n",
    "### What is differencing?\n",
    "- Minusing out the last data point from this data point for every data point\n",
    "\n",
    "$$ y_{t}' = y_t - y_{t-1}$$ \n",
    "\n",
    "More mathematical details can be found here: https://otexts.com/fpp2/stationarity.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting back to ARIMA: AR models\n",
    "- Stands for autoregressive\n",
    "- In essence, we are:\n",
    "> forecasting future variables using a linear combination of past values of the variable\n",
    "- Mathematically speaking:\n",
    "\n",
    "$$ y_t = c + {\\phi}y_{t-1} + {\\phi}y_{t-2} + ... + {\\phi}y_{t-p} + {\\varepsilon}_t $$\n",
    "- AR models are parametrized as $AR(p)$, where $p$ is called the \"order\" or the number of time steps to \"learn from\"\n",
    "- When used to model stationary data, $\\phi$ should be between -1 and 1\n",
    "    - It should be viewed as \"weights\" for how much each previous time step influences the current time step\n",
    "- $c$ is the mean of the data\n",
    "- ${\\varepsilon}_t$ is normally distributed white noise with mean of 0, variance of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AR models - some pictures\n",
    "\n",
    "<img src='imgs/arp-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second part: MA models\n",
    "- Stands for moving average\n",
    "- In essence, we are:\n",
    "> forecasting future variables using past forecast error in a regression-like way\n",
    "- Mathematically speaking:\n",
    "\n",
    "$$ y_t = c + {\\varepsilon}_t + \\theta_1{\\varepsilon}_{t-1} + \\theta_2{\\varepsilon}_{t-2} + ... + + \\theta_q{\\varepsilon}_{t-q}$$\n",
    "- MA models are parametrized as $MA(q)$, where $q$ is the order or time steps to \"learn from\"\n",
    "- For stationarity, $\\theta$ should be between -1 and 1, also to be viewed as weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MA models - some pictures\n",
    "\n",
    "<img src='imgs/maq-1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Putting it all together: ARIMA\n",
    "\n",
    "- The idea is with AR and MA processes put together, with differencing, we can find a fit for the seasonally-adjusted data\n",
    "- An ARIMA model has 3 parameters:\n",
    "\n",
    "$$ \\text{ARIMA}(p, d, q) $$\n",
    "\n",
    "- $p$ is the order for the AR component\n",
    "- $d$ is the order for differencing\n",
    "- $q$ is the order for the MA component\n",
    "\n",
    "### Forecasting\n",
    "- It's about finding the right parameters to fit the seasonally-adjusted data the best\n",
    "- Then, we can add the seasonal component to make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning (not covered in Compass)\n",
    "\n",
    "- I sometimes find this much easier to understand (much less explainability though)\n",
    "- Don't have time to cover, but check out the following resource\n",
    "\n",
    "Resource: https://github.com/TomasBeuzen/machine-learning-tutorials/blob/master/ml-timeseries/notebooks/supervised_time_series_intro.ipynb"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
